{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPduo7973E4xiXJVXbwK4+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shouraykumra/LLM/blob/FineTuningLLM/FineTuning%2BRAG%2B_WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TBxyNhydDoRH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index llama-index-embeddings-huggingface peft auto-gptq optimum bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "uXBKZXLeT522"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding"
      ],
      "metadata": {
        "id": "8tR6XvMUPsXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 256\n",
        "Settings.chunk_overlap = 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R_pA8cxUjWL",
        "outputId": "32b392ef-31f8-4261-ef03-4e0daf0896d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the articles of which words would be embedded to further feed into model for answer generation."
      ],
      "metadata": {
        "id": "AH_7pPPXP6PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = SimpleDirectoryReader(input_files=[\"/content/Scaling.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOKwVJ4NTEvR",
        "outputId": "27ca9433-103e-43ba-ae1a-a1c2aa9f6879"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 17 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 23 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 29 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 31 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 34 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 79 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 81 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 117 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 119 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 121 0 (offset 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pwj-vY_TQNs",
        "outputId": "daa04f46-a2cc-49a4-ecac-857741614ef7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in doc:\n",
        "  d.text = d.text[177:]"
      ],
      "metadata": {
        "id": "Vp0LJiccTwwd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THuG1IUoTz30",
        "outputId": "fc64f960-a72a-491a-ee90-afcb861a639c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### store the documents into vector DB"
      ],
      "metadata": {
        "id": "V1YphLxAXwNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(doc)"
      ],
      "metadata": {
        "id": "YId7ksdzXltZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search Function"
      ],
      "metadata": {
        "id": "y_lmy2m7X6PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 2\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k\n",
        ")"
      ],
      "metadata": {
        "id": "UyZoiRP6X0t6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever, node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
        "  )"
      ],
      "metadata": {
        "id": "tmGV803_Xuy3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is lightning module?'\n",
        "response = query_engine.query(query)\n",
        "context = \"Context:\\n\"\n",
        "\n",
        "for i in range(top_k):\n",
        "  context = context + response.source_nodes[i].text + '\\n\\n'\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUOveiILYQny",
        "outputId": "98231a94-657f-4ff0-a0e0-71cdaf79d22b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "With its battle-tested Trainerand LightningModule, PyTorch Lightning assists in preventing human errors by optimizingand managing the engineering of the model training process.Table of ContentsTraining Billion Parameter Large ModelsCUDA Out of Memory with Large (Language) ModelsOptimizing Large Model TrainingTraining on a single GPUSharding the training on multiple GPUsActivation CheckpointingCPU OffloadingTraining Llama 7BConclusionRelated ContentLightning AI Joins AI Alliance To Advance Open, Safe, Responsible AI\n",
            "Doubling Neural Network Finetuning Efficiency with 16-bit PrecisionTechniques\n",
            "Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds ofExperimentsScaling Large (Language) Models with PyTorch LightningPosted on October 4, 2023 by Aniket Maurya & Aniket Maurya &   -   Blog, Tutorials← BACK TO BLOGLightning AI Studios: Never set up a local environment again →StudiosDocsReleasesCommunityProductsSolutionsAboutPricingLoginStart free\n",
            "\n",
            "But first, we’ll delveinto the FSDP (Fully Sharded Data Parallel) strategy in PyTorch Lightning using a 2Btransformer model. Afterward, we’ll apply our newfound knowledge to train the Llamamodel.Training on a single GPUWe set up a Transformer training script on the WikiText2 dataset. We create aLightningModule named LanguageModel which defines the training step and configureoptimizer. We create a dataloader and configure the Trainer. All the device and trainingstrategy-related arguments are provided to the trainer.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"shouray/YT_Results\")\n",
        "model = PeftModel.from_pretrained(model, \"shouray/YT_Results\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOzNKa2uY-wN",
        "outputId": "e30ed9ed-4ed5-495e-cc23-c33aec97d14a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"What is lightning?\""
      ],
      "metadata": {
        "id": "sienlmbjc6P5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prompt (with context)\n",
        "prompt_template_w_context = lambda context, comment: f\"\"\"[INST]GPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–GPT'. \\\n",
        "GPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "{context}\n",
        "Please respond to the following comment. Use the context above if it is helpful.\n",
        "\n",
        "{comment}\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bv7skpC5hHdm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template_w_context(context, comment)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs['input_ids'].to('cuda'), max_new_tokens=280)\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuxLshViCkj",
        "outputId": "536839b2-5c77-4b0a-cd81-c74b4b7e5a68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST]GPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–GPT'. GPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Context:\n",
            "With its battle-tested Trainerand LightningModule, PyTorch Lightning assists in preventing human errors by optimizingand managing the engineering of the model training process.Table of ContentsTraining Billion Parameter Large ModelsCUDA Out of Memory with Large (Language) ModelsOptimizing Large Model TrainingTraining on a single GPUSharding the training on multiple GPUsActivation CheckpointingCPU OffloadingTraining Llama 7BConclusionRelated ContentLightning AI Joins AI Alliance To Advance Open, Safe, Responsible AI\n",
            "Doubling Neural Network Finetuning Efficiency with 16-bit PrecisionTechniques\n",
            "Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds ofExperimentsScaling Large (Language) Models with PyTorch LightningPosted on October 4, 2023 by Aniket Maurya & Aniket Maurya &   -   Blog, Tutorials← BACK TO BLOGLightning AI Studios: Never set up a local environment again →StudiosDocsReleasesCommunityProductsSolutionsAboutPricingLoginStart free\n",
            "\n",
            "But first, we’ll delveinto the FSDP (Fully Sharded Data Parallel) strategy in PyTorch Lightning using a 2Btransformer model. Afterward, we’ll apply our newfound knowledge to train the Llamamodel.Training on a single GPUWe set up a Transformer training script on the WikiText2 dataset. We create aLightningModule named LanguageModel which defines the training step and configureoptimizer. We create a dataloader and configure the Trainer. All the device and trainingstrategy-related arguments are provided to the trainer.\n",
            "\n",
            "\n",
            "Please respond to the following comment. Use the context above if it is helpful.\n",
            "\n",
            "What is lightning?\n",
            "[/INST]\n",
            "\n",
            "I'd be happy to help clarify!\n",
            "\n",
            "PyTorch Lightning is a deep learning library that simplifies the process of building, training, and deploying deep learning models. It's designed to make the engineering of deep learning models more efficient and less error-prone.\n",
            "\n",
            "In the context of the article, the author is discussing how PyTorch Lightning can be used to train large models, specifically a 2B transformer model and the Llama model. The author mentions that PyTorch Lightning optimizes and manages the engineering of the model training process, which includes optimizing large model training, managing training on a single GPU, and sharding the training across multiple GPUs.\n",
            "\n",
            "So, in short, PyTorch Lightning is a library that makes it easier to build, train, and deploy deep learning models, with a focus on optimizing the engineering process for large models.\n",
            "\n",
            "I hope that helps clarify things a bit! Let me know if you have any other questions.\n",
            "\n",
            "–GPT</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCEZD_vGhnd2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/rag/rag_example.ipynb"
      ],
      "metadata": {
        "id": "-6R8aoXglzd0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFRb5wbpl0yZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}